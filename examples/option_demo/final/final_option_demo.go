package main

import (
	"fmt"
	"runtime"
	"time"

	"github.com/kart-io/goagent/llm"
	"github.com/kart-io/goagent/llm/constants"
	"github.com/kart-io/goagent/llm/providers"
	"github.com/kart-io/goagent/tools"
)

func main() {
	fmt.Println("ğŸš€ GoAgent Option æ¨¡å¼æ¼”ç¤º")
	fmt.Println("=====================================")

	// Demo 1: ç¼“å­˜ Option æ¨¡å¼
	demoCacheOptions()

	// Demo 2: LLM Option æ¨¡å¼
	demoLLMOptions()

	fmt.Println("\n=====================================")
	fmt.Println("âœ… æ¼”ç¤ºå®Œæˆï¼")
	fmt.Println("\næ€»ç»“:")
	fmt.Println("1. Option æ¨¡å¼æä¾›äº†çµæ´»çš„é…ç½®æ–¹å¼")
	fmt.Println("2. é¢„è®¾é…ç½®ç®€åŒ–äº†å¸¸è§ç”¨ä¾‹")
	fmt.Println("3. Builder æ¨¡å¼æä¾›æµç•…çš„ API")
	fmt.Println("4. å·¥å‚æ–¹æ³•ç»Ÿä¸€å®¢æˆ·ç«¯åˆ›å»º")
}

func demoCacheOptions() {
	fmt.Println("ğŸ“Œ Demo 1: ç¼“å­˜ Option æ¨¡å¼")
	fmt.Println("-----------------------------------")

	fmt.Println("\nç¼“å­˜é…ç½®é€‰é¡¹:")

	// ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºç¼“å­˜
	defaultCache := tools.NewShardedToolCacheWithOptions()
	fmt.Println("âœ… é»˜è®¤ç¼“å­˜å·²åˆ›å»º")
	defaultCache.Close()

	// ä½¿ç”¨æ€§èƒ½é…ç½®æ–‡ä»¶
	fmt.Println("\næ€§èƒ½é…ç½®æ–‡ä»¶:")
	performanceCache := tools.NewShardedToolCacheWithOptions(
		tools.WithPerformanceProfile(tools.LowLatencyProfile),
	)
	fmt.Println("  - ä½å»¶è¿Ÿé…ç½®å·²åº”ç”¨")
	performanceCache.Close()

	highThroughputCache := tools.NewShardedToolCacheWithOptions(
		tools.WithPerformanceProfile(tools.HighThroughputProfile),
	)
	fmt.Println("  - é«˜ååé…ç½®å·²åº”ç”¨")
	highThroughputCache.Close()

	// å·¥ä½œè´Ÿè½½ä¼˜åŒ–
	fmt.Println("\nå·¥ä½œè´Ÿè½½ä¼˜åŒ–:")
	readHeavyCache := tools.NewShardedToolCacheWithOptions(
		tools.WithWorkloadType(tools.ReadHeavyWorkload),
	)
	fmt.Println("  - è¯»å¯†é›†å‹é…ç½®å·²åº”ç”¨")
	readHeavyCache.Close()

	writeHeavyCache := tools.NewShardedToolCacheWithOptions(
		tools.WithWorkloadType(tools.WriteHeavyWorkload),
	)
	fmt.Println("  - å†™å¯†é›†å‹é…ç½®å·²åº”ç”¨")
	writeHeavyCache.Close()

	// è‡ªå®šä¹‰é…ç½®
	fmt.Println("\nè‡ªå®šä¹‰é…ç½®:")
	customCache := tools.NewShardedToolCacheWithOptions(
		tools.WithShardCount(64),
		tools.WithCapacity(100000),
		tools.WithCleanupInterval(5*time.Minute),
		tools.WithEvictionPolicy(tools.LRUEviction),
		tools.WithAutoTuning(true),
	)
	fmt.Println("  âœ… è‡ªå®šä¹‰é…ç½®å·²åº”ç”¨:")
	fmt.Println("     - 64 åˆ†ç‰‡")
	fmt.Println("     - å®¹é‡ 100000")
	fmt.Println("     - æ¸…ç†é—´éš” 5 åˆ†é’Ÿ")
	fmt.Println("     - LRU é©±é€ç­–ç•¥")
	fmt.Println("     - è‡ªåŠ¨è°ƒä¼˜å¯ç”¨")
	customCache.Close()

	// æ€§èƒ½å»ºè®®
	fmt.Println("\næ€§èƒ½é…ç½®å»ºè®®:")
	fmt.Printf("  - ä½æµé‡ (<100 QPS): %d åˆ†ç‰‡\n", runtime.NumCPU())
	fmt.Printf("  - ä¸­æµé‡ (100-1K QPS): %d åˆ†ç‰‡\n", runtime.NumCPU()*2)
	fmt.Printf("  - é«˜æµé‡ (1K-10K QPS): %d åˆ†ç‰‡\n", runtime.NumCPU()*4)
	fmt.Printf("  - è¶…é«˜æµé‡ (>10K QPS): %d åˆ†ç‰‡\n", runtime.NumCPU()*8)
}

func demoLLMOptions() {
	fmt.Println("\nğŸ“Œ Demo 2: LLM Option æ¨¡å¼")
	fmt.Println("-----------------------------------")

	// 1. åŸºæœ¬é…ç½®
	fmt.Println("\nåŸºæœ¬é…ç½®:")
	basicConfig := llm.NewLLMOptionsWithOptions(
		llm.WithProvider(constants.ProviderOpenAI),
		llm.WithAPIKey("demo-key"),
		llm.WithModel("gpt-4"),
		llm.WithMaxTokens(2000),
		llm.WithTemperature(0.7),
	)
	fmt.Printf("  Provider: %s\n", basicConfig.Provider)
	fmt.Printf("  Model: %s\n", basicConfig.Model)
	fmt.Printf("  MaxTokens: %d\n", basicConfig.MaxTokens)
	fmt.Printf("  Temperature: %.2f\n", basicConfig.Temperature)

	// 2. é¢„è®¾é…ç½®
	fmt.Println("\né¢„è®¾é…ç½®å¯¹æ¯”:")
	fmt.Println("é¢„è®¾          | Model           | Tokens | Temp | Cache")
	fmt.Println("-------------|-----------------|--------|------|-------")

	presets := []struct {
		name   string
		preset llm.PresetOption
	}{
		{"å¼€å‘", llm.PresetDevelopment},
		{"ç”Ÿäº§", llm.PresetProduction},
		{"ä½æˆæœ¬", llm.PresetLowCost},
		{"é«˜è´¨é‡", llm.PresetHighQuality},
		{"å¿«é€Ÿ", llm.PresetFast},
	}

	for _, p := range presets {
		config := llm.NewLLMOptionsWithOptions(
			llm.WithProvider(constants.ProviderOpenAI),
			llm.WithAPIKey("demo"),
			llm.WithPreset(p.preset),
		)
		cacheStr := "No"
		if config.CacheEnabled {
			cacheStr = fmt.Sprintf("%v", config.CacheTTL)
		}
		fmt.Printf("%-12s | %-15s | %-6d | %.2f | %s\n",
			p.name, config.Model, config.MaxTokens, config.Temperature, cacheStr)
	}

	// 3. ä½¿ç”¨åœºæ™¯ä¼˜åŒ–
	fmt.Println("\nä½¿ç”¨åœºæ™¯ä¼˜åŒ–:")
	fmt.Println("åœºæ™¯     | æ¸©åº¦ | Tokens | TopP | è¯´æ˜")
	fmt.Println("---------|------|--------|------|----------")

	useCases := []struct {
		name    string
		useCase llm.UseCase
		desc    string
	}{
		{"èŠå¤©", llm.UseCaseChat, "è‡ªç„¶å¯¹è¯"},
		{"ä»£ç ", llm.UseCaseCodeGeneration, "ä¸€è‡´è¾“å‡º"},
		{"ç¿»è¯‘", llm.UseCaseTranslation, "å‡†ç¡®ç¿»è¯‘"},
		{"æ‘˜è¦", llm.UseCaseSummarization, "ç®€æ´æ€»ç»“"},
		{"åˆ†æ", llm.UseCaseAnalysis, "è¯¦ç»†åˆ†æ"},
		{"åˆ›ä½œ", llm.UseCaseCreativeWriting, "åˆ›æ„å†…å®¹"},
	}

	for _, uc := range useCases {
		config := llm.NewLLMOptionsWithOptions(
			llm.WithProvider(constants.ProviderOpenAI),
			llm.WithAPIKey("demo"),
			llm.WithUseCase(uc.useCase),
		)
		fmt.Printf("%-8s | %.2f | %-6d | %.2f | %s\n",
			uc.name, config.Temperature, config.MaxTokens, config.TopP, uc.desc)
	}

	// 4. Option æ¨¡å¼ï¼ˆæ¨èï¼‰
	fmt.Println("\nOption æ¨¡å¼ï¼ˆæ¨èï¼‰:")
	client, err := providers.NewOpenAIWithOptions(
		llm.WithAPIKey("demo-key"),
		llm.WithModel("gpt-4"),
		llm.WithTemperature(0.7),
		llm.WithMaxTokens(2000),
		llm.WithRetryCount(3),
		llm.WithRetryDelay(2*time.Second),
		llm.WithCache(true, 10*time.Minute),
	)
	if err != nil {
		fmt.Printf("Error: %v\n", err)
		return
	}
	_ = client

	fmt.Println("  OpenAI Option æ¨¡å¼é…ç½®:")
	fmt.Println("  âœ… API Key è®¾ç½®")
	fmt.Println("  âœ… Model: gpt-4")
	fmt.Println("  âœ… Temperature: 0.7")
	fmt.Println("  âœ… MaxTokens: 2000")
	fmt.Println("  âœ… Retry: 3æ¬¡ï¼Œ2ç§’å»¶è¿Ÿ")
	fmt.Println("  âœ… Cache: 10åˆ†é’Ÿ TTL")

	// 5. é«˜çº§åŠŸèƒ½
	fmt.Println("\né«˜çº§åŠŸèƒ½é…ç½®:")
	advancedConfig := llm.NewLLMOptionsWithOptions(
		llm.WithProvider(constants.ProviderOpenAI),
		llm.WithAPIKey("demo-key"),
		llm.WithModel("gpt-4"),
		llm.WithRetryCount(3),
		llm.WithRetryDelay(2*time.Second),
		llm.WithCache(true, 10*time.Minute),
		llm.WithRateLimiting(100),
		llm.WithSystemPrompt("You are an expert assistant"),
		llm.WithStreamingEnabled(true),
		llm.WithCustomHeaders(map[string]string{
			"X-Request-ID": "123",
		}),
	)

	fmt.Println("  âœ… é‡è¯•æœºåˆ¶: 3 æ¬¡")
	fmt.Println("  âœ… ç¼“å­˜: å¯ç”¨ (10åˆ†é’Ÿ)")
	fmt.Println("  âœ… é€Ÿç‡é™åˆ¶: 100 RPM")
	fmt.Println("  âœ… ç³»ç»Ÿæç¤º: å·²è®¾ç½®")
	fmt.Println("  âœ… æµå¼å“åº”: å¯ç”¨")
	fmt.Println("  âœ… è‡ªå®šä¹‰å¤´: å·²æ·»åŠ ")
	_ = advancedConfig

	// 6. é…ç½®éªŒè¯
	fmt.Println("\né…ç½®éªŒè¯:")

	// æœ‰æ•ˆé…ç½®
	validConfig := &llm.LLMOptions{
		Provider:  constants.ProviderOpenAI,
		APIKey:    "test-key",
		Model:     "gpt-4",
		MaxTokens: 2000,
	}
	if err := llm.PrepareConfig(validConfig); err == nil {
		fmt.Println("  âœ… OpenAI é…ç½®éªŒè¯é€šè¿‡")
	}

	// Ollama ä¸éœ€è¦ API Key
	ollamaConfig := &llm.LLMOptions{
		Provider: constants.ProviderOllama,
		Model:    "llama2",
		BaseURL:  "http://localhost:11434",
	}
	if err := llm.PrepareConfig(ollamaConfig); err == nil {
		fmt.Println("  âœ… Ollama é…ç½®éªŒè¯é€šè¿‡ï¼ˆæ— éœ€ API Keyï¼‰")
	}

	// ç¼ºå°‘ API Key
	invalidConfig := &llm.LLMOptions{
		Provider: constants.ProviderOpenAI,
		Model:    "gpt-4",
	}
	if err := llm.PrepareConfig(invalidConfig); err != nil {
		fmt.Println("  âœ… æ— æ•ˆé…ç½®æ­£ç¡®æ‹’ç»ï¼ˆç¼ºå°‘ API Keyï¼‰")
	}
}
